---
title: "K最近邻(KNN)与支持向量机(SVM)"
author: "Zehui Bai"
date: 'Stand: `r format(Sys.time(), "%F %H:%M Uhr")`'
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---

```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

## 机器学习基于R 第五章
library(class)
library(kknn)
library(e1071)
library(kernlab)
library(caret)
library(MASS)
library(reshape2)
library(ggplot2)
library(pROC)
```


# Introduction

逻辑斯蒂回归，它被用来预测一个观测属于某个响应变量分类的概率—— 我们称之为分类问题。逻辑斯蒂回归只是分类方法的开始，还可以使用很多其他方法改善预测 质量。

两种非线性技术：K最近邻（KNN）与支持向量机（SVM）。这两种技术要比我们之前讨论的那些技术复杂一些，因为放弃了线性假设. 也就是说，不再必须使用特征的线性组合来定义决策边界。

这样不一定能得到更好的预测结果，而且解释模型也会有一点问题，计算效率也更低。正确使用这些技术时，可以作为其他 技术和工具的强有力的补充

## K-Nearest Neighbor，KNN

K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：在特征空间中，如果一个样本附近的k个最近(即特征空间中最邻近)样本的大多数属于某一个类别，则该样本也属于这个类别。k的作用是确定算法应该检查多少个近邻，如果k = 5，算法将检查5个最近的点。这种方法的缺点 是，所有5个点在算法中都被赋予相同的权重

如果k太小，那么测试 集上的观测可能会有很高的方差——尽管偏差很低。另一方面，当k增加时，方差会减小，但偏 差可能会变得不可接受。必须进行交叉验证以确定合适的k值。

另一个需要指出的重要问题是距离的计算，或者说是特征空间中数据点的临近度的计算。默 认的距离是欧氏距离，也就是从点A到点B的简单直线距离——就像乌鸦飞过的直线。你也可以 使用公式计算，欧氏距离等于两点坐标之差的平方和的平方根

KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成反比。该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。

该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。


## Support Vector Machine, SVM

支持向量机（Support Vector Machine, SVM）是一类按监督学习（supervised learning）方式对数据进行二元分类的广义线性分类器（generalized linear classifier），其决策边界是对学习样本求解的最大边距超平面（maximum-margin hyperplane）


下页图说明了这个思想。细实线就是最优线性分类器，它建立了上面提到的最大可能边际， 提高了一个新观测落入分类器正确一侧的概率。两条粗黑线对应着安全边际，阴影数据点构成了 支持向量。如果支持向量发生移动，就会导致边际和决策范围发生改变。分类器之间的距离被称 为边际。


```{r, echo=FALSE, fig.align="center", out.width = '100%',fig.cap="Figure: Ampute Procese"}
knitr::include_graphics("./00 Fotos/SVM.png")
```


如果数据不是线性可分的，很多观测值就会落到分类边际错误的一侧（所谓松弛 变量），这就是误分类。建立SVM算法的关键是，通过交叉验证找出最优数量的 支持向量。任何一个正好位于最大分类边际上的观测都可以被认为是支持向量。

如果误差值的调优参数过大，你就会找到很多支持向量，受到高偏差低方差的困扰。而如果 调优参数过小，就会出现相反的情况。

SVM中的另一个重要问题是处理非线性模型的能力，非线性模型的输入特征带有二次项或更 高阶的多项式。在SVM中，这种处理被称为核技巧。对于任何模型，你都可以通过不同阶数的多项式、交互项或其他衍生项来扩展特征的数量。 在大规模数据集中，这样做可能失控。SVM中的核技巧可以使我们有效扩展特征空间，目的是使 特征空间近似于线性可分. 

SVM最优化问题及其约束条件。我们希望:

找出使边际最大的权值。  
满足约束条件：没有（或尽量少的）数据点位于边际之内。

核函数的巧妙之处在于，它将特征到高维空间的转换进行了数学上的简化，不需要在高维空 间中显式地创建特征。这样做的好处是，在建立高维非线性空间和决策边界的同时，还能保持最 优问题的计算有效性。核函数不用将特征转换到高维空间即可计算特征在高维空间中的内积。一般用特征的内积（点积）表示核函数，用xi和xj代表向量，γ和c为参数，常用的核函数:

* 线性核函数
* 多项式核函数
* 径向基核函数
* sigmod核函数



# Application

们在同一个数据集上应用KNN和SVM, 对混淆矩阵进行深入研究，并对评价 模型正确率的各个统计量进行比较.

要研究的数据来自美国国家糖尿病消化病肾病研究所，这个数据集包括532个观测，8 个输入特征以及1个二值结果变量（Yes/No）。这项研究中的患者来自美国亚利桑那州中南部，是 皮玛族印第安人的后裔。数据显示，在过去的30年中，科学家已经通过研究证明肥胖是引发糖尿 病的重要因素。选择皮玛印第安人进行这项研究是因为，半数成年皮玛印第安人患有糖尿病。而这些患有糖尿病的人中，有95%超重。研究仅限于成年女性，病情则按照世界卫生组织的标准进 行诊断，为Ⅱ型糖尿病。这种糖尿病的患者胰腺功能并未完全丧失，还可以产生胰岛素，因此又 称“非胰岛素依赖型”糖尿病。

是研究那些糖尿病患者，并对这个人群中可能导致糖尿病的风险因素进行预测。 久坐不动的生活方式和高热量的饮食习惯使得糖尿病已经成为美国的流行病。根据美国糖尿病协 会的数据，2010年，糖尿病成为美国排名第七的致死疾病，这个结果还不包括那些未被诊断出来 的病例。糖尿病还会大大增加其他疾病的发病概率，比如高血压、血脂异常、中风、眼疾和肾脏 疾病。糖尿病及其并发症的医疗成本非常巨大，据估计，美国2012年糖尿病治疗总成本大约为4900 亿美元。

## Data Preparation

数据集包含了532位女性患者的信息，存储在两个数据框中。数据集包含在MASS这个R包中，一个数据框是Pima.tr，另一个数据框的是Pima.te。我们不 将它们分别作为训练集和测试集，而是将其合在一起，然后建立自己的训练集和测试集

数据集变量如下:

    npreg：怀孕次数  
    glu：血糖浓度，由口服葡萄糖耐量测试给出  
    bp：舒张压（单位为mm Hg）  
    skin：三头肌皮褶厚度（单位为mm）  
    bmi：身体质量指数  
    ped：糖尿病家族影响因素  
    age：年龄  
    type：是否患有糖尿病（是/否）


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
data(Pima.tr)
str(Pima.tr)
data(Pima.te)
str(Pima.te)
pima <- rbind(Pima.tr, Pima.te)
str(pima)

## 通过箱线图进行探索性分析。为此，要使用结果变量"type"作为ID变量的值。和逻辑斯蒂 回归一样，melt()函数会融合数据并准备好用于生成箱线图的数据框
## 用facet_wrap()函数将统计图分两列显示
pima.melt <- melt(pima, id.var = "type")
ggplot(data = pima.melt, aes(x = type, y = value)) +
  geom_boxplot() + facet_wrap(~ variable, ncol = 2)

## 为很难从中发现任何明显区别, 里最大的问题是，不同统计图的 单位不同，但却共用一个Y轴。对数据进行标准化处理并重新做图，可以解决这个问题，并生成 更有意义的统计图。
## R内建函数scale()，可以将数据转换为均值为0、标准差为1的标准 形式, 你对一个数据框应用了scale()函数，它 就自动变成一个矩阵。使用as.data.frame()函数，将其重新变回数据框
## 要对所有特征进行转换，只留 下响应变量type
pima.scale <- data.frame(scale(pima[, -8]))
#scale.pima = as.data.frame(scale(pima[,1:7], byrow=FALSE)) #do not create own function
str(pima.scale)
pima.scale$type <- pima$type

pima.scale.melt <- melt(pima.scale, id.var = "type")
ggplot(data=pima.scale.melt, aes(x = type, y = value)) + 
  geom_boxplot() + facet_wrap(~ variable, ncol = 2)
## Interpretation: 出其他特征也随着 type发生变化，特别是age

## 有两对变量之间具有相关性：npreg/age和skin/bmi。如果能够正确训练模型，并能调整好 超参数，那么多重共线性对于这些方法通常都不是问题
cor(pima.scale[-8])
## 先检查响应变量中 Yes和No的比例。确保数据划分平衡是非常重要的，如果某个结果过于稀疏，就会导致问题，可 能引起分类器在优势类和劣势类之间发生偏离。对于不平衡的判定没有一个固定的规则。一个比 较好的经验法则是，结果中的比例至少应该达到2∶1
table(pima.scale$type)


## 比例为2∶1，现在可以建立训练集和测试集了。使用我们常用的语法，划分比例为70/30
set.seed(502)
ind <- sample(2, nrow(pima.scale), replace = TRUE, prob = c(0.7, 0.3))
train <- pima.scale[ind == 1, ]
test <- pima.scale[ind == 2, ]
str(train)
str(test)
```



## KNN建模

使用KNN建模关键的一点就是选择最合适的参数（k或K）。在确定k值 方面，caret包又可以大展身手了。先建立一个供实验用的输入网格，k值从2到20，每次增加1。 使用expand.grid()和seq()函数可以轻松实现。在caret包中，作用于KNN函数的参数非常简 单直接，就是.k：


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
grid1 <- expand.grid(.k = seq(2, 20, by = 1))
## 选择参数时，还是使用交叉验证。先建立一个名为control的对象，然后使用caret包中的 trainControl()函数
control = trainControl(method = "cv")

## 先设定随机数种子, ，使用caret包中train()函数建立计算最优k值的对象
## 使用train()函数建立对象时，需要指定模型公式、训练数据集名称和一个合适的方法(knn), 以建立对象并计算最优k值
set.seed(123)
knn.train <- train(type ~ ., data = train, 
                   method = "knn", 
                   trControl = control, 
                   tuneGrid = grid1)
## 调用这个对象即可得到我们追寻的最优k值，是17: The final value used for the model was k = 17.
knn.train
## Interpretation
## 在输出的表格中还可以看到正确率和Kappa统计量的信 息，以及交叉验证过程中产生的标准差。
 # 正确率告诉我们模型正确分类的百分比。
 # Kappa又称科 恩的K统计量，通常用于测量两个分类器对观测值分类的一致性。Kappa可以使我们对分类问题 的理解更加深入，它对正确率进行了修正，去除了仅靠偶然性（或随机性）获得正确分类的因素。 计算这个统计量的公式是Kappa = (一致性百分比 期望一致性百分比)/(1 期望一致性百分比)。 一致性百分比是分类器的分类结果与实际分类相符合的程度（就是正确率），期望一致性百 分比是分类器靠随机选择获得的与实际分类相符合的程度。Kappa统计量的值越大，分类器的分 类效果越好，Kappa为1时达到一致性的最大值。

## 应用到测试数据集: 如何计算正确率和Kappa
knn.test <- knn(train[, -8], test[, -8], train[, 8], k = 17)
table(knn.test, test$type)
## 正确率: 用分类正确的观测数除以观测总数
(77+28)/147
## calculate Kappa
prob.agree <- (77+28)/147
prob.chance <- ((77+26)/147) * ((77+16)/147)
prob.chance
kappa <- (prob.agree - prob.chance) / (1 - prob.chance)
kappa
## 解释Kappa:  ＜0.20 很差; 0.21 ~ 0.40 一般; 0.41 ~ 0.60 中等; 0.61 ~ 0.80 好; 0.81 ~ 1.00 很好
```

## 加权最近邻法

看是否可以使用 加权最近邻法得到更好的结果。加权最近邻法提高了离观测更近的邻居的影响力，降低了远离观 测的邻居的影响力。观测离空间点越远，对它的影响力的惩罚就越大。要使用加权最近邻法，需 要kknn包中的train.kknn()函数来选择最优的加权方式

train.kknn()函数使用我们前面介绍过的LOOCV选择最优参数，比如最优的K最近邻数 量、二选一的距离测量方式，以及核函数。

不加权的K最近邻算法使用的是欧式距离。在kknn包中，除了欧式距离， 还可以选择两点坐标差的绝对值之和。如果要使用这种距离计算方式，需要指定闵可夫斯基距 离参数。

有多种方法可以对距离进行加权, kknn包中有10种不同的加权方式，不加权也 是其中之一。它们是：retangular（不加权）、triangular、epanechnikov、biweight、triweight、consine、 inversion、gaussian、rank和optimal。

赋予权重之前，算法对所 有距离进行标准化处理，使它们的值都在0和1之间。triangular加权方法先算出1减去距离的差， 再用差作为权重去乘这个距离。epanechnikov加权方法是用3/4乘以(1 距离的平方)。


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## 两种加权方法triangular, epanechnikov和标准的不加权方法
## 先指定随机数种子，然后使用kknn()函数建立训练集对象, k值的最大值kmax、距离distance（1表示绝对值距离，2表示欧氏距离）、核函数kernel
set.seed(123)
kknn.train <- train.kknn(type ~ ., data = train, 
                         kmax = 25, distance = 2, 
                         kernel = c("rectangular", "triangular", "epanechnikov"))

## plot中X轴表示的是k值，Y轴表示的是核函数误分类观测百分比
plot(kknn.train)

## 以调用对象看看分类误差和最优参数
kknn.train

## 从上面的数据可以看出，给距离加权不能提高模型在训练集上的正确率。而且从下面的代码 可以看出，它同样不能提高测试集上的正确率
kknn.pred <- predict(kknn.train, newdata = test)
table(kknn.pred, test$type)
```

## SVM建模

使用e1071包构建SVM模型，先从线性支持向量分类器开始，然后转入非线性模型

e1071 包中有一个非常好的用于SVM的函数——tune.svm()，它可以帮助我们选择调优参数及核函 数。tune.svm()使用交叉验证使调优参数达到最优。我们先建立一个名为linear.tune的对象， 然后使用summary()函数看看其中的内容

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## linear tune
set.seed(123)
linear.tune <- tune.svm(type ~ ., data = train, 
                        kernel = "linear", 
                        cost = c(0.001, 0.01, 0.1, 1, 5, 10))
summary(linear.tune)

## 最优成本函数cost是1，这时的误分类误差率差不多为21%。我们在测试集 上进行预测和检验
best.linear <- linear.tune$best.model
tune.test <- predict(best.linear, newdata = test)
table(tune.test, test$type)
(82+30)/147

## 试验的第一个核函数是多项式核函数，需要调整优化两个参数：多项式的阶（degree） 与核系数（coef0）。设定多项式的阶是3、4和5，核系数从0.1逐渐增加到4
## SVM with e1071; tune the poly only
set.seed(123)
poly.tune <- tune.svm(type ~ ., data = train, 
                      kernel = "polynomial", 
                      degree = c(3, 4, 5), 
                      coef0 = c(0.1, 0.5, 1, 2, 3, 4))
summary(poly.tune)
best.poly <- poly.tune$best.model
poly.test <- predict(best.poly, newdata = test)
table(poly.test, test$type)
(81 + 26) / 147


## 测试径向基核函数，此处只需找出一个参数gamma， 在0.1 ~ 4中依次检验。如果gamma过小，模型就不能解释决策边界的复杂性；如果gamma过大， 模型就会严重过拟合。
## tune the rbf
set.seed(123)
rbf.tune <- tune.svm(type ~ ., data = train, 
                     kernel = "radial", 
                     gamma = c(0.1, 0.5, 1, 2, 3, 4))
summary(rbf.tune)
best.rbf <- rbf.tune$best.model
rbf.test <- predict(best.rbf, newdata = test)
table(rbf.test, test$type)
(73+21)/147


## 找出两个参 数——gamma和核系数（coef0）
## tune the sigmoid
set.seed(123)
sigmoid.tune <- tune.svm(type ~ ., data = train, 
                         kernel = "sigmoid", 
                         gamma = c(0.1, 0.5, 1, 2, 3, 4),
                         coef0 = c(0.1, 0.5, 1, 2, 3, 4))
summary(sigmoid.tune)
best.sigmoid <- sigmoid.tune$best.model
sigmoid.test <- predict(best.sigmoid, newdata = test)
table(sigmoid.test, test$type)
(82+35)/147
## 在测试集上表现得更好, 可以选 择sigmoid核函数作为最优预测。
## 研究了两种不同类型的建模技术，从各方面来看，KNN都处于下风。KNN在测试 集上最好的正确率只有71%左右，相反，通过SVM可以获得接近80%的正确率(使用sigmoid核函数的SVM模型)。
```

## 模型选择

通过混淆矩阵来比较各种模型, 友caret包的confusionMatrix() 函数, 使用过InformationValue包中的同名函数。但caret包中的这 个函数会生成我们评价和选择最优模型所需的所有统计量。先从建立的最后一个模型开始，使用 的语法和基础的table()函数一样，不同之处是要指定positive类

其他统计量介绍:

    No Information Rate：最大分类所占的比例——63%的人没有糖尿病
    Mcnemar's Test：我们现在不关心这个统计量，它用于配对分析，主要用于流行病学 的研究
    Sensitivity：敏感度，真阳性率；在本案例中，表示没有糖尿病并且被正确识别的 比例。
    Specificity：特异度，真阴性率；在本案例中，表示有糖尿病并且被正确识别的比例
    Pos Pred Value：阳性预测率，被认为有糖尿病的人中真的有糖尿病的概率。
      PPV =敏感度 *患病率/((敏感度 *患病率) + (1-敏感度) * (1-患病率))
    Neg Pred Value：阴性预测率，被认为没有糖尿病的人中真的没有糖尿病的概率
      NPV=敏感度 * (1-患病率)/(((1-敏感度) * (患病率)) + (敏感度) * (1-患病率) )
    Prevalence：患病率，某种疾病在人群中流行度的估计值: 第二列   （Yes列）中的数之和除以总观测数（矩阵中所有数之和）。 
    Detection Rate：真阳性预测中被正确识别的比例
    Detection Prevalence：预测的患病率，在本案例中，底行中的数的和除以总观测数。
    Balanced Accuracy：所有类别正确率的平均数。用来表示由于分类器算法中潜在的偏     差造成的对最频繁类的过度预测。可以简单地用(敏感度 + 特异度)/2来计算

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
confusionMatrix(sigmoid.test, test$type, positive = "Yes")
## 结果和线性SVM进行对比
confusionMatrix(tune.test, test$type, positive = "Yes")
```



## 特征选择

此处忽略了一件事，即没有进行任何特征选择。我们做的工作就是把特征堆在一起， 作为所谓的输入空间，然后让SVM这个黑盒去计算，最后给出一个预测分类。使用SVM的一个 主要问题就是，它给出的结果非常难以解释. 使用caret包进行粗略的特征选择。因为对于那些像SVM一样使用 黑盒技术的方法来说，特征选择确实是个艰巨的挑战。这也是使用这些技术时可能遇到的主要困 难

还有一些其他办法可以进行特征选择, 需要做的就是反复实验。再次用到caret包，因为它可以基于kernlab包在线性SVM中 进行交叉验证. 

设定随机数种子，在caret包中的rfeControl()函数中指定交叉验 证方法，使用rfe()函数执行一个递归的特征选择过程，最后检验模型在测试集上的运行情况。 在rfeControl()中，你需要根据使用的模型指定functions参数。可以使用几种不同的functions 参数，此处使用lrFuncs


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
set.seed(123)
rfeCNTL <- rfeControl(functions = lrFuncs, method = "cv", number = 10)
## 要指定输入数据和响应因子、通过参数sizes指定输入 特征的数量以及kernlab包中的线性方法（此处是svmLinear）。method还有其他一些选项
svm.features <- rfe(train[, 1:7], train[, 8],
                   sizes = c(7, 6, 5, 4), 
                   rfeControl = rfeCNTL, 
                   method = "svmLinear")
svm.features
svm.5 <- svm(type ~ glu + ped + npreg + bmi + age, 
             data = train, 
             kernel = "linear")
svm.5.predict = predict(svm.5, newdata=test[c(1,2,5,6,7)])
table(svm.5.predict, test$type)
## 全特征模型的正确率是76.2%, 表现不怎么样, 要回到全特征模型。
```

