---
title: "主成分分析"
author: "Zehui Bai"
date: 'Stand: `r format(Sys.time(), "%F %H:%M Uhr")`'
output:
  html_document:
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---

```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2) #support scatterplot
# library(GPArotation) #support rotation
library(psych) #PCA package
```




# Introduction

* 聚类分析，它可以将相似的观测 归成一类
* 主成分分析(PCA)，它可以对相关变量进行归类，从而降低 数据维度，提高对数据的理解

维数灾难，因为模型估计所需的样本数量是随着输入特征的 数量指数增长的。这种数据集中会出现某些变量冗余，因为这些变量最后起的作用与其他变量基 本是重复的。比如收入水平与贫穷程度，或者抑郁度与焦虑度。那么我们的目标就是，通过PCA 从原始变量集合中找出一个更小的，但是能保留原来大部分信息的变量集合。这样可以简化数据 集，并经常能够发现数据背后隐藏的知识。这些新的变量(主成分)彼此高度不相关，除了可以 9 用于监督式学习之外，还经常用于数据可视化。

成分就是特征的规范化线性组合(James，2012)。在一个数据集中，第一主成分就 是能够最大程度解释数据中的方差的特征线性组合。第二主成分是另一种特征线性组合，它在方 向与第一主成分垂直这个限制条件下，最大程度解释数据中的方差。其后的每一个主成分(可以 构造与变量数相等数目的主成分)都遵循同样的规则。

Annahmen：

* 线性组合：如果你试图在一个变量之间 基本不相关的数据集上使用PCA，很可能会得到一个毫无意义的分析结果
* 变量的均值和方差是充分统计量。也就是说，数据应该服从正态分布，这样协方差矩阵即可充分 描述数据集。换言之，数据要满足多元正态分布。PCA对于非正态分布的数据具有相当强的鲁棒 性，甚至可以和二值变量一起使用，所以结果具有很好的解释性。

如何精简主成分，来达到降低数据维度这一首要初始目标:

在特征值大于1的情况下选择主成分

最优线性权重是通过线性代数运算得到特征向量而求出的，它们是最优解，因为 没有其他可能的权重组合可以比它们更好地解释方差。主成分的特征值是它在整个数据集中能够解释的方差的数量。


```{r echo=TRUE}
knitr::include_graphics("./00 Fotos/Principal component analysis.png")
```

因为第一特征值可以解释最大数量的方差，它就有最大的特征值;第二主成分有第二大的特 征值，依此类推。所以，特征值大于1就表示这个主成分解释的方差比任何一个原始变量都要大。 如果通过标准化操作将特征值的总和变为1，就能够得到每个主成分解释的方差的比例。这也有 助于确定一个适当的分界点。












## 主成分旋转

旋转可以修改每个变量的载荷，这样有助于对主成分的解释。 旋转后的成分能够解释的方差总量是不变的，但是每个成分对于能够解释的方差总量的贡献会改 变。在旋转过程中，你会发现载荷的值或者更远离0，或者更接近0，这在理论上可以帮助我们识 别那些对主成分起重要作用的变量。这是一种将变量和唯一一个主成分联系起来的尝试。

最常用的主成分旋转方法被称为方差最大法。在方差最大法中，我们要使平方后的载荷的总方差最大。方差最大化过程会旋转 特征空间的轴和坐标，但不改变数据点的位置。








# Application

## 数据准备

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
test <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R_lernen/00 Data/NHLtest.csv", header=T)
train <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R_lernen/00 Data/NHLtrain.csv", header=T)

str(train)
names(train)

train.scale <- scale(train[, -1:-2])
nhl.cor = cor(train.scale)
# dev.off()
cor.plot(nhl.cor)
```






## 模型构建

对于模型构建过程，我们按照以下几个步骤进行:

* (1) 抽取主成分并决定保留的数量; 
* (2) 对留下的主成分进行旋转;
* (3) 对旋转后的解决方案进行解释; 
* (4) 生成各个因子的得分;
* (5) 使用得分作为输入变量进行回归分析，并使用测试数据评价模型效果。



### 主成分抽取

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## 通过psych包抽取主成分要使用principal()函数，这个函数的语法中要包括数据和是否要 进行主成分旋转:
pca <- principal(train.scale, rotate="none")
## 使用碎石图即可。碎石图可以帮助你评估能解释大部分数据方差的主成分，它用X轴表示 主成分的数量，用Y轴表示相应的特征值:
plot(pca$values, type="b", ylab="Eigenvalues", xlab="Component")
## 需要在碎石图中找出使变化率降低的那个点，也就是我们常说的统计图中的“肘点”或弯曲 点。在统计图中，肘点表示在这个点上新增加一个主成分时，对方差的解释增加得并不太多。换 句话说，这个点就是曲线由陡变平的转折点
```


### 正交旋转与解释

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## 旋转背后的意义是使变量在某个主成分上的载荷最大化，这样可以减少(或消灭)主成分之间的相关性，有助于对主成分的解释。进行正交旋转的方法称为“方差最大法”。还有其他非正交旋转方法，这种方法允许主成分(因子)之间存在相关性
## 设定使用5个主成分，并需要进行正交旋转
pca.rotate <- principal(train.scale, nfactors = 5, rotate = "varimax")
pca.rotate
```

### 根据主成分建立因子得分

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
pca.scores <- data.frame(pca.rotate$scores)
head(pca.scores)
## 得到每个球队在每个因子上的得分，这些得分的计算非常简单，每个观测的变量值乘以载荷 然后相加即可。现在可以将响应变量(ppg)作为一列加入数据
pca.scores$ppg <- train$ppg
```


### 回归分析

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
nhl.lm <- lm(ppg ~ ., data = pca.scores)
summary(nhl.lm)
nhl.lm2 <- lm(ppg ~ RC1 + RC2, data = pca.scores)
summary(nhl.lm2)

plot(nhl.lm2$fitted.values, train$ppg, 
     main="Predicted versus Actual",
     xlab="Predicted",ylab="Actual")

## 使用ggplot2包生成一张带有球队名 字的散点图
train$pred <- round(nhl.lm2$fitted.values, digits = 2)
p <- ggplot(train, aes(x = pred,
                           y = ppg,
                           label = Team)) 
p + geom_point() + 
  geom_text(size=3.5, hjust=0.1, vjust=-0.5, angle=0) + 
  xlim(0.8, 1.4) + ylim(0.8, 1.5) +
  stat_smooth(method="lm", se=FALSE)



## 评价模型误差
sqrt(mean(nhl.lm2$residuals^2))



## 模型在样本外数据上的效果
test.scores <- data.frame(predict(pca.rotate, test[, c(-1:-2)]))
test.scores$pred <- predict(nhl.lm2, test.scores)

test.scores$ppg <- test$ppg
test.scores$Team <- test$Team

p <- ggplot(test.scores, aes(x = pred,
                       y = ppg,
                       label = Team)) 
p + geom_point() + 
  geom_text(size=3.5, hjust=0.4, vjust = -0.9, angle = 35) + 
  xlim(0.75, 1.5) + ylim(0.5, 1.6) +
  stat_smooth(method="lm", se=FALSE)

resid <- test.scores$ppg - test.scores$pred
sqrt(mean(resid^2))
```


