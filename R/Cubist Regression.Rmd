---
title: ''
author: "Zehui Bai"
date: 'Stand: `r format(Sys.time(), "%F %H:%M Uhr")`'
output:
  html_document:
    df_print: paged
    number_sections: No
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
fontsize: 10pt
editor_options:
  chunk_output_type: console
colorlinks: yes
---

```{r setup, include=FALSE, echo = FALSE,message = FALSE, error = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# load package

```





# Application

## Data Preparation


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
library("Cubist")
library("mlbench")

## Data Preparation

data(BostonHousing)
BostonHousing$chas <- as.numeric(BostonHousing$chas) - 1

set.seed(1)
inTrain <- sample(1:nrow(BostonHousing), floor(.8*nrow(BostonHousing)))

## Predictors
train_pred <- BostonHousing[ inTrain, -14]
test_pred  <- BostonHousing[-inTrain, -14]

## Responder variable
train_resp <- BostonHousing$medv[ inTrain]
test_resp  <- BostonHousing$medv[-inTrain]
```


## Fit Continious Outcome

The modelTree method for Cubist shows the usage of each variable in either the rule conditions or the (terminal) linear model. In actuality, many more linear models are used in prediction that are shown in the output. Because of this, the variable usage statistics shown at the end of the output of the summary function will probably be inconsistent with the rules also shown in the output. At each split of the tree, Cubist saves a linear model (after feature selection) that is allowed to have terms for each variable used in the current split or any split above it. Quinlan (1992) discusses a smoothing algorithm where each model prediction is a linear combination of the parent and child model along the tree. As such, the final prediction is a function of all the linear models from the initial node to the terminal node. The percentages shown in the Cubist output reflects all the models involved in prediction (as opposed to the terminal models shown in the output).

> Cubist的modelTree方法显示了规则条件或（最终）线性模型中每个变量的用法。实际上，在预测中使用了更多的线性模型，这些模型显示在输出中。因此，摘要功能输出末尾显示的变量使用情况统计信息可能与输出中也显示的规则不一致。在树的每个分割处，Cubist都会保存一个线性模型（在特征选择之后），该线性模型允许对当前分割处或其上方任何分割处使用的每个变量都具有术语。 Quinlan（1992）讨论了一种平滑算法，其中每个模型预测都是沿着树的父模型和子模型的线性组合。这样，最终预测是从初始节点到终端节点的所有线性模型的函​​数。立体派输出中显示的百分比反映了预测中涉及的所有模型（与输出中显示的终端模型相对）。

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Continious Outcome
## Building mOdel
model_tree <- cubist(x = train_pred, y = train_resp)
summary(model_tree)


## Make the prediction on the test datasets
Cubist.probs <- predict(model_tree, test_pred)

## Test set RMSE
sqrt(mean((Cubist.probs - test_resp)^2))

## Test set R^2
cor(Cubist.probs, test_resp)^2
```

### Variable Importance
 
the variable importance is a linear combination of the usage in the rule conditions and the model.





### Summary display

The tidyRules function in the tidyrules package returns rules in a tibble (an extension of dataframe) with one row per rule. The tibble provides these information about the rule: support, mean, min, max, error, LHS, RHS and committee. 

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
library("tidyrules")
tr <- tidyRules(model_tree)
tr

tr[, c("LHS", "RHS")]
```

### specific parts 

These results can be used to look at specific parts of the data. For example, the 4th rule predictions are:

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
library("rlang")
library("dplyr")
char_to_expr <- function(x, index = 1, model = TRUE) {
  x <- x %>% dplyr::slice(index) 
  if (model) {
    x <- x %>% dplyr::pull(RHS) %>% rlang::parse_expr()
  } else {
    x <- x %>% dplyr::pull(LHS) %>% rlang::parse_expr()
  }
  x
}

rule_expr  <- char_to_expr(tr, 4, model = FALSE)
model_expr <- char_to_expr(tr, 4, model = TRUE)
```
  
  
## Ensembles By Committees 

The Cubist model can also use a boosting–like scheme called committees where iterative model trees are created in sequence. The first tree follows the procedure described in the last section. Subsequent trees are created using adjusted versions to the training set outcome: if the model over–predicted a value, the response is adjusted downward for the next model (and so on). Unlike traditional boosting, stage weights for each committee are not used to average the predictions from each model tree; the final prediction is a simple average of the predictions from each model tree.

> 其中按顺序创建迭代模型树。 第一棵树遵循最后一部分中描述的过程。 使用对训练集结果的调整后的版本来创建后续树：如果模型预测值过高，则针对下一个模型向下调整响应（依此类推）。 与传统的提升不同，每个委员会的阶段权重不会用于平均每个模型树的预测； 最终预测是来自每个模型树的预测的简单平均值。

### Nearest–neighbors Adjustmemt

Another innovation in Cubist using nearest–neighbors to adjust the predictions from the rule–based model. First, a model tree (with or without committees) is created. Once a sample is predicted by this model, Cubist can find it’s nearest neighbors and determine the average of these training set points.

> 使用最近邻来调整基于规则的模型中的预测。 首先，创建一个模型树（有或没有委员会）。 通过该模型预测样本后，Cubist可以找到其最近的邻居，并确定这些训练设定点的平均值。

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Ensembles By Committees 
set.seed(1)
com_model <- cubist(x = train_pred, y = train_resp, committees = 5)
summary(com_model)


## Nearest–neighbors Adjustmemt
inst_pred <- predict(com_model, test_pred, neighbors = 5)
## RMSE
sqrt(mean((inst_pred - test_resp)^2))
## R^2
cor(inst_pred, test_resp)^2
```


## Optimize parameters

To tune the model over different values of neighbors and committees, the train function in the `caret package can be used to optimize these parameters. 

```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}
## Optimize parameters
library("caret")
grid <- expand.grid(committees = c(1, 10, 50, 100),
                    neighbors = c(0, 1, 5, 9))
set.seed(1)
boston_tuned <- train(
  x = train_pred,
  y = train_resp,
  method = "cubist",
  tuneGrid = grid,
  trControl = trainControl(method = "cv")
)
boston_tuned

## The profiles of the tuning parameters 
ggplot(boston_tuned)
```



## Logistic CV


```{r,echo = T,message = FALSE, error = FALSE, warning = FALSE}

library("MASS")
data(biopsy)
biopsy$ID = NULL
names(biopsy) = c("thick", "u.size", "u.shape", "adhsn", 
                  "s.size", "nucl", "chrom", "n.nuc", "mit", "class")
biopsy.v2 <- na.omit(biopsy)

## 用0表示良性，用1表示恶性
y <- ifelse(biopsy.v2$class == "malignant", 1, 0)

set.seed(123) #random number generator
ind <- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3))
train <- biopsy.v2[ind==1, ] #the training data set
test <- biopsy.v2[ind==2, ] #the test data set

trainY <- y[ind==1]
testY <- y[ind==2]



set.seed(123)
ctrl=(trainControl(method="repeatedcv", repeats=5))
c<-c(100)
n<-c(3,4)

cubit.fit<-train(as.matrix(train[-10]),
                 trainY, 
                 method="cubist",
                 preProcess = c("center", "scale"),
                 tuneGrid = expand.grid(committees=c,neighbors=n),
                 trControl = ctrl)
                
summary(cubit.fit)
dotPlot(varImp(cubit.fit), main="Cubist Predictor importance")
```


