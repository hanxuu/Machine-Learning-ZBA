[["ML.html", "Machine Learning Intro Sub analysis 1 Bayesian Theory 1.1 Introduction of Bayesian 2 3 IRT 3.1 Item Response Theory 4 ^_^ Space 5 ^_^ Space 6 Neural Network 6.1 Introduction 6.2  6.3 Application", " Machine Learning Zehui Bai 27 June, 2021 Intro Placeholder Sub analysis Yet another analysis 1 Bayesian Theory 1.1 Introduction of Bayesian 1.1.1 Frequency A set of random samples, the frequency school believes that the overall parameters are constant, and the samples are obtained randomly; The Bayesian school believes that the overall parameters are random, and the sample obtained is constant. The Bayesian school does not care much about the correct parameters, but needs to obtain the posterior by adding the acquired data to the prior knowledge 1.1.2 Posterior distribution The posterior distribution summarises our uncertainty over the value of a parameter. If the distribution is narrower, then this indicates that we have greater confidence in our estimates of the parameters value. More narrow posterior distributions can be obtained by collecting more data. The posterior probability is the probability of the parameters \\(\\theta\\) given the evidence \\(X: p(\\theta \\mid X)\\) It contrasts with the likelihood function, which is the probability of the evidence given the parameters: \\(p(X \\mid \\theta)\\) The two are related as follows: Given a prior belief that a probability distribution function is \\(p(\\theta)\\) and that the observations \\(x\\) have a likelihood \\(p(x \\mid \\theta)\\), then the posterior probability is defined as \\[ p(\\theta \\mid x)=\\frac{p(x \\mid \\theta)}{p(x)} p(\\theta) \\] 2 3 IRT Placeholder 3.1 Item Response Theory 3.1.1 CTT and IRT 4 ^_^ Space 5 ^_^ Space 6 Neural Network Placeholder 6.1 Introduction 6.2  6.3 Application 6.3.1  6.3.2  "]]
